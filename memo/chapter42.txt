Seq2Seq (Sequence to Sequence)
    - 기본 개념: 입력 시퀀스를 출력 시퀀스로 변환하는 모델. 주로 기계 번역, 자동 요약, 대화 생성 등에 사용.
    - 구조:
        - 인코더: 입력 시퀀스를 단일 벡터(컨텍스트 벡터)로 인코딩.
        - 디코더: 인코딩된 벡터를 입력으로 받아 출력 시퀀스 생성.
Seq2Seq의 학습 및 예측 과정
    - 인코더 학습: 입력 시퀀스 각 단어를 RNN을 통해 처리, 마지막 히든 스테이트가 컨텍스트 벡터로 사용됨.
    - 디코더 학습: 시작 토큰(<sos>)을 시작으로, 인코더의 컨텍스트 벡터와 이전 시점의 출력을 입력으로 받아 다음 단어 예측.
    - Teacher Forcing: 실제 출력 단어를 다음 시점의 입력으로 사용하는 방법.
    - Cross-Entropy Loss: 디코더 출력의 손실 계산에 사용.
Seq2Seq 모델의 활용
    - 기계 번역: 하나의 언어를 다른 언어로 번역.
    - 대화 시스템: 주어진 질문에 대한 대답 생성.
    - 텍스트 요약: 긴 텍스트를 요약문으로 변환.