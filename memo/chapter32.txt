Word2Vec 소개
    - 개념: Word2Vec은 단어의 연속적인 벡터 표현을 생성하는 모델로, 두 가지 주요 구조(CBOW, Skip-gram)를 포함.
    - 목적: Word2Vec은 단어 간의 유사성을 벡터 공간에서 표현함으로써, 단어의 의미론적, 문법적 관계를 포착.
Word2Vec의 특징
    - 간결한 모델 구조: 비교적 간단한 구조이지만 높은 성능을 제공.
    - 연산 효율성: 복잡한 신경망 모델에 비해 낮은 연산 복잡도를 가진다.
모델 아키텍처
    - CBOW (Continuous Bag of Words): 주변 단어를 기반으로 특정 단어를 예측.
    - Skip-gram: 하나의 단어에서 주변 단어를 예측.
    - 훈련 과정: Stochastic Gradient Descent와 Backpropagation을 사용하여 학습.
Word2Vec의 응용
    - 유사 단어 찾기: 특정 단어와 유사한 단어를 벡터 연산을 통해 찾아낼 수 있다.
    - 단어 간 관계 파악: 예를 들어, "프랑스:파리 = 독일:베를린"과 같은 의미 관계를 벡터 연산으로 표현 가능.
평가 및 비교
    - 성능 평가: Word2Vec는 의미적, 문법적 문항 테스트를 통해 다른 모델과 비교.
한글 Word2Vec
    - 한국어 데이터에 대한 Word2Vec 적용 사례도 포함되어 있다.