4. TF-IDF (Term Frequency-Inverse Document Frequency)
    - 개념: 문서 내 단어의 중요성을 수치화하는 방법. 단어 빈도(TF)와 역문서 빈도(IDF)를 결합해 각 단어의 가중치를 계산
    - 계산 절차:
        - TF 계산: 문서 내에서 각 단어가 등장하는 횟수를 측정. 이는 단어가 문서 내에서 얼마나 자주 등장하는지를 표현
        - IDF 계산: 전체 문서 집합에서 특정 단어가 얼마나 희귀한지 측정. 단어가 등장한 문서 수의 역수에 로그를 취하여 계산. 흔한 단어는 낮은 IDF 값을, 드문 단어는 높은 IDF 값을 가짐.
        - TF-IDF 계산: TF 값과 IDF 값을 곱하여 최종 TF-IDF 점수를 도출. 이 값이 높을수록 해당 문서에 대해 더 중요한 단어로 간주.
    - 적용: TF-IDF 점수가 높은 단어는 해당 문서에서 중요하거나 독특한 정보를 제공한다. 검색 엔진 최적화, 문서 분류, 정보 검색 등에 활용.

5. n-Gram
    - 개념: 연속된 n개의 단어를 하나의 토큰으로 간주하고, 이를 통해 제한적인 문맥을 포착하는 방법.
    - 계산 절차:
        - 토큰 Index 생성: 문서 내의 모든 유니크한 단어에 고유한 인덱스를 부여. 이 인덱스는 단어를 수치화하고 처리하기 위한 기초.
        - n-Gram 생성: 문서 내에서 연속적으로 등장하는 n개의 단어를 묶어 하나의 토큰으로 취급. 예를 들어, bigram(n=2)의 경우 "the cat", "cat sat", "sat on" 등과 같이 인접한 단어 쌍을 사용.
    - 한계: n-Gram의 크기(n) 선택에 따라 성능이 달라짐. 너무 큰 n은 모델이 복잡해지고, 드문 n-Gram이 많아지면서 OOV 문제가 발생할 수 있다. 반대로 n이 너무 작으면 문맥 정보가 충분히 포착되지 않을 수 있다.
    - 적용 분야: n-Gram은 특히 언어 모델링, 텍스트 분류, 검색 엔진에서 유용하게 사용. 특정 분야의 언어 스타일과 문맥을 포착하는 데 도움.
- TF-IDF와 n-Gram 모두 텍스트 데이터를 분석하고 처리하는 데 있어 핵심적인 기술로, TF-IDF는 단어의 중요성을, n-Gram은 문맥 정보를 포착하는 데 각각 강점을 가지고 있다.
- 분야(Domain)에 따라 단어들의 확률 분포는 다름(금융 분야는 금융 관련 용어가 많이 등장하고, 마케팅은 관련 용어가 많이 등장)
- 분야에 적합한 코퍼스를 사용하면 언어 모델의 성능이 높아질 수 있음(훈련에 사용되는 코퍼스에 따라 언어 모델의 성능이 달라짐. 이는 언어 모델의 약점으로 분류되기도 함)

* Google Books Ngram Viewer : https://books.google.com/